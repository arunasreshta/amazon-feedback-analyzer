# -*- coding: utf-8 -*-
"""feedback_analyzer_ollama_cli.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/174PL-bUmR0YlAy4sScRBXUt8W_Fa8akq
"""

# =============================================================
# Amazon Review Analyzer with Ollama CLI
# =============================================================

import pandas as pd
import re
import numpy as np
from sklearn.cluster import KMeans
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from tqdm import tqdm
import subprocess
import os
from sklearn.manifold import TSNE

# --- Optional: suppress HuggingFace tokenizer parallelism warnings ---
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from sentence_transformers import SentenceTransformer
import time

# --- Step 1: Load & clean ---
def load_and_clean(path, max_rows=None):
    """Load CSV, drop missing text, and clean text."""
    df = pd.read_csv(path)
    if 'Text' not in df.columns:
        raise ValueError("CSV must contain a 'Text' column.")
    df = df.dropna(subset=['Text'])
    if max_rows:
        df = df.head(max_rows)
    df['clean_text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', ' ', str(x)).lower().strip())
    return df

# --- Step 2: Compute embeddings safely in batches ---
def get_embeddings_chunked_safe(texts, chunk_size=250):
    """Compute embeddings in manageable batches to avoid memory issues."""
    all_embeddings = []
    total = len(texts)
    num_batches = (total // chunk_size) + 1
    model = SentenceTransformer('all-MiniLM-L6-v2')

    for i in tqdm(range(num_batches), desc="Embedding batches safely"):
        start, end = i * chunk_size, min((i + 1) * chunk_size, total)
        batch = texts[start:end]
        if not batch:
            continue
        for attempt in range(3):  # retry if batch fails
            try:
                emb = model.encode(batch, show_progress_bar=False, convert_to_numpy=True)
                all_embeddings.append(emb)
                break
            except Exception as e:
                print(f"⚠️ Batch {i} failed on attempt {attempt+1}: {e}")
                time.sleep(3)
        else:
            print(f" Skipping batch {i} after repeated failure")
    return np.vstack(all_embeddings)

# --- Step 3: Cluster embeddings ---
def create_clusters(embeddings, num_clusters=5):
    """Perform KMeans clustering on embeddings."""
    km = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)
    return km.fit_predict(embeddings)

# --- Step 4: Summarize via Ollama CLI ---
def ollama_summarize(texts, model="llama3.2", max_chars=8000):
    """Call Ollama CLI to summarize text in 3 bullet points."""
    if not texts:
        return "(No text to summarize)"
    combined = " ".join(texts)
    if len(combined) > max_chars:
        combined = combined[:max_chars]

    prompt = f"Summarize the following customer feedback in 3 bullet points:\n\n{combined}"

    try:
        result = subprocess.run(
            ["ollama", "run", model],
            input=prompt.encode("utf-8"),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=180
        )
        output = result.stdout.decode("utf-8").strip()
        if not output:
            print("Ollama returned empty output. STDERR:", result.stderr.decode("utf-8"))
            return "(No summary generated)"
        return output
    except subprocess.TimeoutExpired:
        print("Ollama summarization timed out.")
        return "(Summarization timed out)"
    except Exception as e:
        print(f"Ollama summarization failed: {e}")
        return "(Summarization failed)"

# --- Step 5: Word Cloud Visualization ---
def plot_wordcloud(texts, title):
    """Generate and display a word cloud from a list of texts."""
    text_combined = " ".join(texts)
    wc = WordCloud(width=800, height=400, background_color='white').generate(text_combined)
    plt.figure(figsize=(10,5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

# --- Main pipeline ---
def run_feedback_analysis(csv_path, num_clusters=5, top_n=5, max_rows=None):
    df = load_and_clean(csv_path, max_rows=max_rows)

    # --- Step 2: embeddings ---
    embeddings = get_embeddings_chunked_safe(df['clean_text'].tolist(), chunk_size=500)
    df['embedding'] = list(embeddings)  # save embeddings for t-SNE

    # --- Step 3: clustering ---
    df['cluster'] = create_clusters(embeddings, num_clusters)

    # --- Step 4: summarize ---
    cluster_summaries = {}
    for c in range(num_clusters):
        texts = df[df['cluster']==c]['clean_text'].head(top_n).tolist()
        if texts:
            cluster_summaries[c] = ollama_summarize(texts)
        else:
            cluster_summaries[c] = ""

    return df, cluster_summaries

# --- Run ---
if __name__ == "__main__":
    CSV_FILE = "/Users/arunasreshta/Documents/Amazon data feedbacks/Reviews.csv"
    df_out, summaries = run_feedback_analysis(CSV_FILE, num_clusters=5, top_n=5, max_rows=5000)

    # === Cluster Summaries ===
    print("\n=== Cluster Summaries ===")
    for c, s in summaries.items():
        print(f"\nCluster {c} Summary:\n{s}\n{'='*60}")

    # === Word Clouds per cluster ===
    print("\n=== Generating Word Clouds ===")
    for c in sorted(df_out['cluster'].unique()):
        plot_wordcloud(df_out[df_out['cluster'] == c]['clean_text'].tolist(), f"Cluster {c}")

    # === Cluster Distribution Bar Chart ===
    plt.figure(figsize=(7, 4))
    df_out['cluster'].value_counts().sort_index().plot(kind='bar', color='darkcyan', edgecolor='black')
    plt.title("Feedback Volume per Cluster")
    plt.xlabel("Cluster")
    plt.ylabel("Number of Feedbacks")
    plt.tight_layout()
    plt.show()

    # === 2D t-SNE Visualization ===
    embeddings_array = np.vstack(df_out['embedding'].values)
    tsne = TSNE(n_components=2, perplexity=40, random_state=42, init="pca", learning_rate="auto")
    tsne_results = tsne.fit_transform(embeddings_array)

    plt.figure(figsize=(8, 6))
    plt.scatter(tsne_results[:, 0], tsne_results[:, 1],
                c=df_out['cluster'], cmap='tab10', s=35, alpha=0.8)
    plt.title("Feedback Clusters (t-SNE Projection)")
    plt.xlabel("Dimension 1")
    plt.ylabel("Dimension 2")
    plt.tight_layout()
    plt.show()

    # === Save clustered results ===
    out_path = "clustered_feedback_summary.csv"
    df_out.to_csv(out_path, index=False)
    print(f"\nResults saved to {out_path}")

    # === Quick summary per cluster ---
    print("\n--- Concise Overview ---")
    for c, s in summaries.items():
        first_line = s.split(".")[0] if s else "No summary generated."
        print(f"Cluster {c} → {first_line}")

# === Save CSV ===
out_csv_path = "outputs/clustered_feedback_summary.csv"
df_out.to_csv(out_csv_path, index=False)
print(f"\n Results saved to {out_csv_path}")

# === Save Word Clouds ===
for c in sorted(df_out['cluster'].unique()):
    wc_texts = df_out[df_out['cluster'] == c]['clean_text'].tolist()
    wc = WordCloud(width=800, height=400, background_color='white').generate(" ".join(wc_texts))
    fig_path = f"outputs/figures/cluster_{c}_wordcloud.png"
    wc.to_file(fig_path)
    print(f" Word cloud saved to {fig_path}")

# === Save t-SNE plot ===
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

embeddings = np.vstack(df_out['embedding'].values)
tsne = TSNE(n_components=2, perplexity=40, random_state=42, init="pca", learning_rate="auto")
tsne_results = tsne.fit_transform(embeddings)

plt.figure(figsize=(8,6))
plt.scatter(tsne_results[:,0], tsne_results[:,1], c=df_out['cluster'], cmap='tab10', s=35, alpha=0.8)
plt.title("Feedback Clusters (t-SNE Projection)")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
tsne_path = "outputs/figures/tsne_clusters.png"
plt.savefig(tsne_path)
plt.close()
print(f" t-SNE plot saved to {tsne_path}")